{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Instructional Materials\n",
    "\n",
    "These instructional materials are presented in the form of a [Jupyter Notebook](https://jupyter.org). Jupyter is a platform for interactive computing. The Notebook is a document format that allow for reading and writing both code and human-readable text.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this module is to demonstrate how to use computational methods to work with Open Civic Data to answer a data driven question as discussed in [Module 5.1: Asking Data Driven Questions](https://civic-switchboard.gitbook.io/education-series/segment-5-using-community-data/module-5.1-asking-data-driven-questions).\n",
    "\n",
    "There are many ways you can use computation to manipulate data. Spreadsheet applications like Microsoft Excel or Google Sheets are popular because of their ease of use.\n",
    "\n",
    "In this module, we are going to demonstrate how to work with open civic data using the Python programming language.\n",
    "\n",
    "*Why Python?* By using a programming language we can *show our work* by writing code to perform data manipulations, rather than interacting with a graphical user interfaces.\n",
    "\n",
    "One of the key things to understand is we are *programming for insight* rather than writing a program to create software.\n",
    "\n",
    "### Caveats\n",
    "\n",
    "This notebook has been designed to you to read and follow the computations by executing the code cells as you progress through the notebook. You do not need to write your own code or understand Python programming to follow the materials. The activities involve modifying code, but the instructions for how to do so are included.\n",
    "\n",
    "If you are interested in learning more about Python programming, we recommend the [Python 4 Everybody](https://www.py4e.com) book and online course as a starting point.\n",
    "\n",
    "The goal of this module is to demonstrate the programmatic manipulation and transformation of data. These materials will discuss what the Python is doing from a conceptual perspective rather than provide a detailed explaination of the code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Data Driven Question\n",
    "\n",
    "In this module we are going to answer the question: What neighborhood in Pittsburgh uses the most Wifi at a Public Library?\n",
    "\n",
    "To fully answer this question we are going to need to work with three datasets:\n",
    "- [CLP Public Wifi](https://data.wprdc.org/dataset/clp-public-wifi)\n",
    "- [CLP Library Locations](https://data.wprdc.org/dataset/libraries)\n",
    "- [Pittsburgh Neighborhoods](https://data.wprdc.org/dataset/neighborhoods2)\n",
    "\n",
    "Answering this question requires the following manipulations:\n",
    "  - **Joining** - If we look at the CLP Public Wifi data, the dataset has no information about neighborhoods. We need to enrich the data with additional information about neighborhoods by *joining* it to other datasets.\n",
    "  - **Aggregation** - Each datapoint of the CLP Public Wifi data represents an individual session. There are a lot of sessions! The data needs to be aggregated to be more comprehensible. This requires some kind of mathematical operation to combine the datapoints together."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading the CLP Public Wifi Data\n",
    "\n",
    "As discussed in [Module 3.5: File Formats for Open Civic Data](https://civic-switchboard.gitbook.io/education-series/segment-3-preparing-libraries-for-sharing-their-data/module-3.5-file-formats-for-open-civic-data), it is important for the data to be downloadable in an open, machine-readable format. The WPRDC provides the CLP Public Wifi data a variety of file formats.\n",
    "* CSV - Comma Separated Values\n",
    "* TSV - Tab Separated Values\n",
    "* JSON - JavaScript Object Notation\n",
    "* XML - eXtensible Markup Language\n",
    "\n",
    "In each of these different files the data is the same, but the way it is structured is different. CSV is the most common format for saving tabular data as a file.\n",
    "\n",
    "If you click and download the CLP Public Wifi as CSV you should note, the default filename is a very confusing string of numbers and letters, `20843d56-506f-44b1-83df-5b16ee865783.csv` This is an automatically generated filename by the WPRDC database, but we are going to rename the file `clp-public-wifi.csv` to make it easier to use.\n",
    "\n",
    "The data file has already been downloaded and placed next to the notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the Data\n",
    "\n",
    "When we download the CLP Public Wifi data to our local computer, this gives us the ability to open and work with the dataset in whatever applications that are installed in our computer. Often the default application for working with tabular data in CSV files is Microsoft Excel. In our case, we will be using the Python programming language to open and manipulate the data file.\n",
    "\n",
    "Loading Pandas is like opening the Excel application on our computer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the Python libraries for working with data\n",
    "import pandas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can use the pandas `read_csv` function to load the data from our hard drive into memory so it can be manipulated with Python code. When we load the data in Python we need to assign the data to a variable, in this case we use a variable called `wifi_data`.\n",
    "\n",
    "If we were using Excel, this would be like opening the CSV file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the csv and save the data in a variable called wifi_data\n",
    "wifi_data = pandas.read_csv(\"clp-public-wifi.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It worked! But we don't see anything!? Working with data in Python is a bit different then using a graphical interface like Excel. With Python all of our interactions are text based commands rather than clicking with the mouse. So if we want to see the data we need to tell Python we want to look at it.\n",
    "\n",
    "Using the `head` function we can tell Python to display the top or \"head\" of the tabular dataset. The default is five rows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display the first 5 rows of the data\n",
    "wifi_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usually, you don't want to display every row in a dataset because many datasets are so large they will overflow your screen! If we want to get a sense of the size of our dataset, we can use the `len()` function to determine the \"length\" (number of rows) in our data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# return the number of rows in the data\n",
    "len(wifi_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This output is simply a number and fortunately this matches up with what the data looked like when we looked at it on the [WPRDC website](https://data.wprdc.org/dataset/clp-public-wifi/resource/20843d56-506f-44b1-83df-5b16ee865783?view_id=251a99da-b3b5-4cdd-a64c-716dcf80275d), 532 rows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answering Data Driven Questions\n",
    "\n",
    "Now that the data has been loaded into Python with Pandas, we are able to perform calculations to learn more about the data. Even with this dataset we can begin to learn a bit about wifi usage at the Carnegie Libraries of Pittsburgh."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Total WiFi Minutes\n",
    "\n",
    "For example, what is the total usage of wifi minutes at all CLP locations? To answer this question, we can add together all of the values in the `WifiMinutes` column in the data. This *sum* will represent the total number of minutes the wifi has been used at all CLP locations over the period of time represented by the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the sum of the WifiMinutes column\n",
    "wifi_data[\"WifiMinutes\"].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's a lot of minutes! How many hours, days, years would that be? To answer this question, we must take that total number of minutes and perform a series of mathematical calculations to determine hours, days, and years."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute total minutes and save in a variable\n",
    "total_minutes = wifi_data[\"WifiMinutes\"].sum()\n",
    "\n",
    "# compute the number of hours\n",
    "total_hours = total_minutes / 60\n",
    "print(\"Total Hours:\", total_hours)\n",
    "\n",
    "# compute the number of days\n",
    "total_days = total_hours / 24\n",
    "print(\"Total Days:\", total_days)\n",
    "\n",
    "# compute the number of years\n",
    "total_years = total_days / 365\n",
    "print(\"Total Years:\", total_years)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "WOW! That is a lot of years! Over 200!? That seems like a lot. Remember, this number represents the cumulative amount of time people have been using the internet at all of the CLP library locations.\n",
    "\n",
    "This leads to another question, how many people have been using the public Wifi at CLP? How much time do they spend on the internet?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the total number of wifi sessions by calculating the sum of the WifiSessions column\n",
    "wifi_data[\"WifiSessions\"].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this number tells us how many times someone connected to the public Wifi. With this information we could get a sense of how long people are using the internet every time they connect."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the total sessions and save to a variable\n",
    "total_sessions = wifi_data[\"WifiSessions\"].sum()\n",
    "\n",
    "# compute the average number of minutes for teach session\n",
    "total_minutes / total_sessions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So this means, on average, people used the wireless internet for about two hours and 12 minutes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recreating the WPRDC Chart\n",
    "\n",
    "If you visit the [CLP Public Wifi](https://data.wprdc.org/dataset/clp-public-wifi) web page, you will see a shart showing Wifi usage for 2017. We can recreate that chart using Python!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify the year in a variable\n",
    "year = 2017\n",
    "\n",
    "# Select data for specified year, group by library/month, and aggregate by the adding together the minutes\n",
    "wifi_data_subset = wifi_data[wifi_data['Year'] == year].groupby([\"Name\", \"Month\"], as_index=False)[\"WifiMinutes\"].sum()\n",
    "\n",
    "# Reshape the data so it is easier to plot by Month\n",
    "reshaped_wifi_data_subset = wifi_data_subset.pivot_table(index=\"Month\", columns=\"Name\", values=\"WifiMinutes\")\n",
    "\n",
    "# plot the data\n",
    "ax = reshaped_wifi_data_subset.plot(figsize=(10,8),\n",
    "                                  title=f\"Carnegie Library of Pittsburgh {year} Wi-Fi Usage in Minutes\",\n",
    "                                  colormap=\"tab20\",\n",
    "                                  fontsize=14)\n",
    "# clean up the text\n",
    "ax.xaxis.label.set_size(16)\n",
    "ax.title.set_size(20)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.6,1))\n",
    "ax.ticklabel_format(style=\"plain\")\n",
    "\n",
    "# add the Month abreviations instead of numbers\n",
    "ax.set_xticks(ticks=range(1,13),labels=[\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]);\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Activity*\n",
    "\n",
    "Modify the code below to create a chart for 2016 and 2018.\n",
    "\n",
    "In the code below, change the number `2017` to be `2016` and then `2018` and then consider the questions below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify the year in a variable\n",
    "year = 2017\n",
    "\n",
    "# Select data for specified year, group by library/month, and aggregate by the adding together the minutes\n",
    "wifi_data_subset = wifi_data[wifi_data['Year'] == year].groupby([\"Name\", \"Month\"], as_index=False)[\"WifiMinutes\"].sum()\n",
    "\n",
    "# Reshape the data so it is easier to plot by Month\n",
    "reshaped_wifi_data_subset = wifi_data_subset.pivot_table(index=\"Month\", columns=\"Name\", values=\"WifiMinutes\")\n",
    "\n",
    "# plot the data\n",
    "ax = reshaped_wifi_data_subset.plot(figsize=(10,8),\n",
    "                                    title=f\"Carnegie Library of Pittsburgh {year} Wi-Fi Usage in Minutes\",\n",
    "                                    colormap=\"tab20\",\n",
    "                                    fontsize=14)\n",
    "# clean up the text\n",
    "ax.xaxis.label.set_size(16)\n",
    "ax.title.set_size(20)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.6,1))\n",
    "ax.ticklabel_format(style=\"plain\")\n",
    "\n",
    "# add the Month abreviations instead of numbers\n",
    "ax.set_xticks(ticks=range(1,13),labels=[\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider the following questions:\n",
    "- What do we see about these other charts?\n",
    "- Are there meaningful differences in the different years?\n",
    "- What is happening in the 2018 data? What do you think that means?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joining Datasets Together\n",
    "\n",
    "To answer our motivating question, *What neighborhood uses the most WiFi?*, we need additional data. As we have seen above, there is no neighborhood information in the CLP Public WiFi data. In fact, there is very little information about any of the libraries.\n",
    "\n",
    "Fortunately, the Carnegie Library of Pittsburgh has posted another dataset on the WPRDC that has a bunch of information about each library location.\n",
    "\n",
    "The [Library Locations](https://data.wprdc.org/dataset/libraries) dataset includes a bunch of information about each CLP branch including \"address, phone number, square footage, and standard operating hours.\" Unfortunately, it does not include is neighborhood information, but it does have GPS locations for library!\n",
    "\n",
    "The WPRDC hosts another dataset, [Pittsburgh Neighborhoods](https://data.wprdc.org/dataset/neighborhoods2), which is a geographic dataset representing the spatial area of each of the 90 neighborhoods in Pittsburgh. What is useful about this dataset is that you can use computational methods to determine in which spatial area a particula GPS coordinate resides. What this means is we can use the GPS data from the library locations dataset and programmatically determine the neighborhood for each library location.\n",
    "\n",
    "By performing these computations we can add a new column to the library locations dataset that includes the neighborhood name. In this way we are **joining** two datasets together to enrich one of the datasets with information from the other."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading the Library Locations\n",
    "\n",
    "The WPRDC also hosts a [Library Locations](https://data.wprdc.org/dataset/libraries/resource/14babf3f-4932-4828-8b49-3c9a03bae6d0?view_id=f34cd02e-17eb-40aa-8f86-ae51968db84a) dataset that includes a bunch of information about each of the libraries in the CLP system.\n",
    "\n",
    "This dataset has 19 entries for each of the 19 libraries. For each library, we have the following pieces of information:\n",
    "\n",
    "- CLPID\n",
    "- Name\n",
    "- Address\n",
    "- City\n",
    "- Zip4\n",
    "- County\n",
    "- Phone\n",
    "- SqFt\n",
    "- The opening times for each day of the week\n",
    "- The closing times for each day of the week\n",
    "- Latitude\n",
    "- Longitude\n",
    "\n",
    "Most of the data in this dataset is location information for each of the locations, but it also includes some information about the library itself, namely how big and when it is open."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the library locations dataset into the variable library_data\n",
    "library_data = pandas.read_csv(\"clp-library-locations.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display the first five rows of the library location data\n",
    "library_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding out Neighborhoods\n",
    "\n",
    "Unfortunately, the library location information is missing a vital piece of information we need: neighborhoods!\n",
    "\n",
    "If we want to obtain neighborhood information for each library, we could spend some time and manually look up it up for each of the 19 locations. But we can also use Python programming to automate the process of looking up each library location and determining the neighborhood. To do this we will use the GPS location of each library location and look it up in a geographic dataset of Pittsburgh's neighborhoods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading the Neighborhood Data\n",
    "\n",
    "* WPRDC publishes a dataset of Pittsburgh Neighborhoods.\n",
    "* [Pittsburgh Neighborhoods](https://data.wprdc.org/dataset/neighborhoods2)\n",
    "* This is a *geographic* dataset which means it requires some special Python libraries for opening and working with the data\n",
    "\n",
    "We need 3rd party library called [shapely](http://shapely.readthedocs.io) which will be used to encode GPS coordinates into geographic points. Then we will use the [Geopandas](https://geopandas.org/en/stable/) library to perform joining operations to determine the neighborhood for each GPS coordinate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load up the necessary geographic libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from numpy import nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Data\n",
    "\n",
    "You need to have a geographic dataset that represents the units of interest. In the current directory is a [geojson](https://en.wikipedia.org/wiki/GeoJSON) file, `neighborhoods.geojson` that encodes all the neighborhoods in Pittsburgh. If you want to data analysis and visualization with neighborhoods, you should copy this file to your workspace."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the neighborhood geojson file\n",
    "pgh_neighborhoods = gpd.read_file(\"neighborhoods.geojson\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Looking at Geographic Data\n",
    "\n",
    "Geopandas gives Python the ability to understand and process geographic datasets. This means we can perform *join* and *aggregation* operations on the data, like we did above with the CLP Wifi Data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# take a peak at what these data look like\n",
    "pgh_neighborhoods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most common way to visualize geographic data is to make maps. The visualization below is just another representation of the neighborhoods' data. Note, the color is being used only to signify a different neighborhood, it doesn't have any particular value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the map using standard pandas plotting functions\n",
    "pgh_neighborhoods.plot(figsize=(10,10), cmap=\"tab20\"); #add semicolon to prevent ugly output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joining the Library Locations and Neighborhoods datasets\n",
    "\n",
    "Pandas has a lot of handy built-in functions, but it does not include a function that can perform the geographic query we demonstrated above. Instead, what we need to do is use a special dataframe function, `apply()` that allows us to create our own custom function that will be *applied* to every row of the data. Apply is kinda like using a `for` loop over every row of your dataframe, but it operates a bit faster.\n",
    "\n",
    "We need to create a python function that does the following:\n",
    "* Take a row as an input parameter\n",
    "* Convert the latitude and longitude columns of that row to a single \"Point\" object\n",
    "* Perform a lookup in the pgh_neighborhoods data to see if that Point/Tree exists\n",
    "* Return the name of the neighborhood if it exists or return `NaN`\n",
    "\n",
    "The function should be written as if it were inside a for loop that iterated over each row of the data, but `apply()` will handle all the looping by calling our function repeatedly and pass each row as a parameter to the function. This is preferable to looping looping over the datafarme in vanilla python because Pandas.\n",
    "\n",
    "What `apply()` will then return is a new Series of neighborhood names derived from the geolocation code. Then we can compare this list of neighborhoods with the actual neighborhood values in the tree data to see if it worked properly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a function that we will supply to apply\n",
    "def reverse_geolocate_neighborhood(row):\n",
    "    \"\"\"Given a row, grab the latitude and longitude columns and\n",
    "    return the neighborhood name (or nan for locations outside the dataset).\"\"\"\n",
    "\n",
    "    # get the latitude and longitude\n",
    "    latitude = float(row['Lat'])\n",
    "    longitude = float(row['Lon'])\n",
    "\n",
    "    # create a shapely point from the GPS coordinates\n",
    "    location = Point(longitude, latitude)\n",
    "\n",
    "    # make a query mask and query the data on that location\n",
    "    location_query = pgh_neighborhoods['geometry'].contains(location)\n",
    "    result = pgh_neighborhoods[location_query]\n",
    "\n",
    "    # if the location isn't in dataset it will be empty\n",
    "    if result.empty:\n",
    "        # location isn't within Pittsburgh, return not-a-number\n",
    "        return nan\n",
    "    else:\n",
    "        # return a string of the \"hood\" where the point was located\n",
    "        return result.iloc[0]['hood']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can give our `reverse_goelocate_neighborhood` function as a parameter to the [dataframe apply](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) function and Pandas process every row of the data using our custom function. This will create a new Pandas series with all the neighborhood names.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# perform reverse geocoding with every row in the library data\n",
    "# save results as a new column in our tree dataframe\n",
    "library_data['neighborhood'] = library_data.apply(reverse_geolocate_neighborhood, axis=1)\n",
    "\n",
    "# display the update librar data\n",
    "library_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neat! Now we have added a neighborhood column to the Library Locations dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have added neighborhood information to our library location data, now what we need to do is connect the library location data with the public wifi data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Joining the Library Locations and Public Wifi Data\n",
    "\n",
    "We now have all of the information we need to answer the question, what neighborhood uses the most Wifi in Pittsburgh. However, the data we need to answer this question is in two separate datasets. What we need to do is *join* the data together."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# look at the public wifi data\n",
    "wifi_data.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# look at the library data\n",
    "library_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When joining together two separate datasets, you need to consider several factors. First, are there any shared data values, that is, are there columns that are the same in both datasets. For the Wifi and Library Locations data we can see there are two shared columns: `CLPID` and `Name`. The data values in the `Name` columns are the same meaning we can join rows from one dataset with rows from another dataset. Second, we need to consider the *thing* that each row represents within each dataset and what kinds of relationships those things have with each other. In the Public Wifi datasets each row represents the wifi usage at a particular library location for a particular month. In the Library Locations dataset each row represents one of the 19 CLP library locations. You can already see how the two datasets are linked if you consider the number of rows mathematically. The Public Wifi datset spans from January 2016 to April 2018 or 28 months. 28 months x 19 libraries equals 532 rows.\n",
    "\n",
    "In the language of data modeling, considering these relationships is called [*cardinality*](https://en.wikipedia.org/wiki/Cardinality_(data_modeling)). There are several ways we can talk about the cardinality, i.e. the relationship between the rows of these two datasets:\n",
    "- One-to-One : A row in one dataset correspond only to a single row in the other dataset. A person has only one library card at their local public library.\n",
    "- One-to-Many: A row in one dataset corresponds to multiple rows in the other dataset. A person can check out many books from their local public library.\n",
    "- Many-to-Many: Multiple rows in one dataset correspond to multiple rows in the other dataset. Many people check out many different books from their local public library.\n",
    "\n",
    "In our case, we have a one-to-many relationship between Library Locations and Public Wifi datasets.\n",
    "\n",
    "Merging operations are complicated. It is not simply a matter of copying columns from one dataset into another, we need to make sure the new columns align with the data in each row. Fortunately, we can use the `merge` operation in our Python code to automatically join the two datasets together with the correct alignment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# merge the two datasets and save the results into a new variable\n",
    "wifi_with_location_info = pandas.merge(wifi_data, library_data)\n",
    "wifi_with_location_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a new dataset that looks like the Public Wifi dataset, but it now includes location information for each row. If you look at the output above, you will notice there is a lot of repeated values. This is because we did a \"one to many\" join, that is, the data from one row in the Library Locations dataset corresponds to many rows in the Public Wifi datasets. While this results in a lot of duplicated data value, if you scroll all the way to the right in the output above you will see there is now a `neighborhood` column with a value for each of the Wifi session.\n",
    "\n",
    "We now have all the information we need to answer our motivating question in one single dataset! The final step is to *aggregate* the data by performing calculations to determine the totals per neighborhood."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split-Apply-Combine\n",
    "\n",
    "When we performed an aggregation operation earlier in this notebook we just computed the sum total number of sessions and minutes across the whole dataset. By adding neighborhood information to the dataset, we can now perform the aggregation operation not just on all the data, but on specific subsets or *groups* of the data. In our case, we want to group rows by the neighborhood and then aggregate the total values for each of these groups.\n",
    "\n",
    "In the language of data analysis, this set of opperations is known as [*split, apply, and combine*](https://pandas.pydata.org/docs/user_guide/groupby.html):\n",
    "> - Splitting the data into groups based on some criteria.\n",
    "> - Applying a function to each group independently.\n",
    "> - Combining the results into a data structure."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create new dataset of total wifi usage per neighborhood using the groupby operation\n",
    "totals_per_neighborhood = wifi_with_location_info.groupby(\"neighborhood\",as_index=False)[[\"WifiSessions\", \"WifiMinutes\"]].sum()\n",
    "totals_per_neighborhood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Python code above did a lot of calculations in very little code. Following the split-apply-combine paradigm we can explain what the code was doing:\n",
    "- **Split** - Separated the `wifi_with_location_info` dataset into 19 groups, one group for each library location's neighborhood.\n",
    "- **Apply** - Selected just the `WifiSessions` and `WifiMinutes` columns and then calculated the sum total value of those columns for each of the groups.\n",
    "- **Combine** - Created a new dataset with 19 rows, one row for each of the grouping values and the single summary value applied to each group.\n",
    "\n",
    "After those operations, we have created a new dataset that contains the answer to our question. However, it is a bit difficult to read so the final step in our computations will be to sort the data based on the aggregated `WifiMinutes` per neighborhood column so we can see who uses the most Wifi."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sort data by WifiMinutes\n",
    "totals_per_neighborhood.sort_values(\"WifiMinutes\", ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see the answer to our question, What neighborhood in Pittsburgh uses the most Wifi at a Public Library? The answer is North Oakland."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Activity*\n",
    "How big are the library locations?\n",
    "\n",
    "\n",
    "Step 1 - Look at the data\n",
    "Run the cell below and look at all the columns, is there a particular column that might help us understand the size of each library location?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display the library locations dataset\n",
    "library_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2 - Select the columns of interest\n",
    "In the code cell below, add the name of the column that will help us determine the size of each library location."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "size_column_name = \"ENTER COLUMN NAME HERE\"\n",
    "\n",
    "library_data[[\"Name\", size_column_name]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Go back and look at the rankings of Public Wifi usage. Consider the following questions:\n",
    "- How does it compare with the size of each library location?\n",
    "- Are there differences in the order of these rankings? If so, why do you think that might be the case?\n",
    "- Consider the communities these library locations serve and how their needs may differ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}