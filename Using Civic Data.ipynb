{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Instructional Materials\n",
    "\n",
    "These instructional materials are presented in the form of a [Jupyter Notebook](https://jupyter.org). Jupyter is a platform for interactive computing. The Notebook is a document format that allow for reading and writing both code and human-readable text. Jupyter Notebooks are composed of *cells*. Each cell can be [Markdown](https://daringfireball.net/projects/markdown/) text or Python code. Markdown cells are for writing human readable content and the code cells are for writing code! \n",
    "\n",
    "**Instructions on using the Interactive Notebook**\n",
    "\n",
    "If you are running the interactive version of this notebook then you can execute the blocks of Python code interactively. When you see a block of Python code, first select the code cell by clicking on it. Once selected a cell will be surrounded by a blue line. You can execute a selected cell by selecting \"Run Selected Cells\" in the *Run* menu at the top of the window, by clicking the play button in the menu bar at the top of the window, or by pressing the shift-return keys on your keyboard.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The purpose of this module is to demonstrate how to use computational methods to work with Open Civic Data to answer a data driven question as discussed in [Module 5.1: Asking Data Driven Questions](https://civic-switchboard.gitbook.io/education-series/segment-5-using-community-data/module-5.1-asking-data-driven-questions).\n",
    "\n",
    "There are many ways you can use computation to manipulate data. Spreadsheet applications like Microsoft Excel or Google Sheets are popular because of their ease of use. However, it is also possible to manipulate data using computer programming to automate the same processes that can be done with spreadsheets. In this module, we are going to demonstrate how to work with open civic data using the Python programming language.\n",
    "\n",
    "*Why Python?* By using a programming language we can *show our work* by writing code to perform data manipulations, rather than interacting with a graphical user interfaces where the history of our clicks are lost in the moment.\n",
    "\n",
    "### Caveats\n",
    "\n",
    "This notebook has been designed to you to read and follow the computations by executing the code cells as you progress through the notebook. You do not need to write your own code or understand Python programming to follow the materials. The activities involve modifying code, but the instructions for how to do so are included.\n",
    "\n",
    "If you are interested in learning more about Python programming, we recommend the [Python 4 Everybody](https://www.py4e.com) book and online course as a starting point.\n",
    "\n",
    "The goal of this module is to demonstrate the programmatic manipulation and transformation of data. These materials will discuss what the Python is doing from a conceptual perspective rather than provide a detailed explanation of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking a Data Driven Question\n",
    "\n",
    "In this module we are going to answer the question: *What neighborhood in Pittsburgh uses the most Wifi at a Public Library?*\n",
    "\n",
    "To fully answer this question we are going to need to work with three datasets:\n",
    "- [CLP Public Wifi](https://data.wprdc.org/dataset/clp-public-wifi)\n",
    "- [CLP Library Locations](https://data.wprdc.org/dataset/libraries)\n",
    "- [Pittsburgh Neighborhoods](https://data.wprdc.org/dataset/neighborhoods2)\n",
    "\n",
    "Answering this question requires the following manipulations:\n",
    "  - **Joining** - Joining operations merge separate datasets together by finding shared values. If we look at the CLP Public Wifi data, the dataset has no information about neighborhoods. We need to enrich the Wifi data with additional columns by *joining* it with other datasets.\n",
    "  - **Aggregation** - Aggregation operations will reduce the size of a dataset by reducing multiple values in rows and/or columns into a single value. Each datapoint of the CLP Public Wifi data represents an individual session. There are a lot of sessions! The data needs to be aggregated to be more comprehensible. This requires a mathematical operation to combine the datapoints together and produce a single, meaningful value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the CLP Public Wifi Data\n",
    "\n",
    "As discussed in [Module 3.5: File Formats for Open Civic Data](https://civic-switchboard.gitbook.io/education-series/segment-3-preparing-libraries-for-sharing-their-data/module-3.5-file-formats-for-open-civic-data), it is important for the data to be downloadable in an open, machine-readable format. The WPRDC provides the CLP Public Wifi data in the CSV or Comma Separate Values file format. CSV is the most common format for saving tabular data as a file.\n",
    "\n",
    "If you click and download the CLP Public Wifi as CSV you should note, the default filename is a very confusing string of numbers and letters, `20843d56-506f-44b1-83df-5b16ee865783.csv` This is an automatically generated filename by the WPRDC database.\n",
    "\n",
    "We have already downloaded and renamed the CLP Public Wifi data and included it as part of these instructional materials. You can double click on the file `clp-public-wifi.csv` in the file browser on the left side of the window to take a look at the data file in Jupyter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "When we download the CLP Public Wifi data to our local computer, this gives us the ability to open and work with the dataset in an application of our choosing. Often the default application for working with tabular data in the CSV file format is Microsoft Excel. In our case, we will be using Jupyter and the Python programming language to open and manipulate our data.\n",
    "\n",
    "To start, we must load a Python library called [Pandas](https://pandas.pydata.org/). Pandas is a data analysis library for the Python programming language that makes working with tabular data easy and fun! Loading Pandas is like opening the Excel application on our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load the Python libraries for working with data\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the pandas `read_csv` function to load the data from our hard drive into memory so it can be manipulated with Python code. When we load the data in Python we need to assign the data to a variable, in this case we use a variable called `wifi_data`.\n",
    "\n",
    "If we were using Excel, this would be like opening the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# read the csv and save the data in a variable called wifi_data\n",
    "wifi_data = pandas.read_csv(\"clp-public-wifi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! But we don't see anything!? Working with data in Python is a bit different then using a graphical interface like Excel. With Python all of our interactions are text based commands rather than clicking with the mouse. So if we want to see the data we need to tell Python we want to look at it.\n",
    "\n",
    "Using the `head` function we can tell Python to display the top or \"head\" of the tabular dataset. The default is five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# display the first 5 rows of the data\n",
    "wifi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, you don't want to display every row in a dataset because many datasets are so large they will overflow your screen! If we want to get a sense of the size of our dataset, we can use the `len()` function to determine the \"length\" (number of rows) in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# return the number of rows in the data\n",
    "len(wifi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is simply a number and fortunately this matches up with what the data looked like when we looked at it on the [WPRDC website](https://data.wprdc.org/dataset/clp-public-wifi/resource/20843d56-506f-44b1-83df-5b16ee865783?view_id=251a99da-b3b5-4cdd-a64c-716dcf80275d), 532 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Data Driven Questions\n",
    "\n",
    "Now that the data has been loaded into Python with Pandas, we are able to perform calculations to learn more about the data. Even with this dataset we can begin to learn a bit about wifi usage at the Carnegie Libraries of Pittsburgh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total WiFi Minutes\n",
    "\n",
    "For example, what is the total usage of wifi minutes at all CLP locations? To answer this question, we can add together all of the values in the `WifiMinutes` column in the data. This *sum* will represent the total number of minutes the wifi has been used at all CLP locations over the period of time represented by the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# compute the sum of the WifiMinutes column\n",
    "wifi_data[\"WifiMinutes\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of minutes! How many hours, days, years would that be? To answer this question, we must take that total number of minutes and perform a series of mathematical calculations to determine hours, days, and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# compute total minutes and save in a variable\n",
    "total_minutes = wifi_data[\"WifiMinutes\"].sum()\n",
    "\n",
    "# compute the number of hours\n",
    "total_hours = total_minutes / 60\n",
    "print(\"Total Hours:\", total_hours)\n",
    "\n",
    "# compute the number of days\n",
    "total_days = total_hours / 24\n",
    "print(\"Total Days:\", total_days)\n",
    "\n",
    "# compute the number of years\n",
    "total_years = total_days / 365\n",
    "print(\"Total Years:\", total_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! That is a lot of years! Over 200!? That seems like a lot. Remember, this number represents the cumulative amount of time people have been using the internet at all of the CLP library locations.\n",
    "\n",
    "This leads to another question, how many people have been using the public Wifi at CLP? How much time do they spend on the internet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# compute the total number of wifi sessions by calculating the sum of the WifiSessions column\n",
    "wifi_data[\"WifiSessions\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this number tells us how many times someone connected to the public Wifi. With this information we could get a sense of how long people are using the internet every time they connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# compute the total sessions and save to a variable\n",
    "total_sessions = wifi_data[\"WifiSessions\"].sum()\n",
    "\n",
    "# compute the average number of minutes for teach session\n",
    "total_minutes / total_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means, on average, people used the wireless internet for about two hours and 12 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating the WPRDC Chart\n",
    "\n",
    "If you visit the [CLP Public Wifi](https://data.wprdc.org/dataset/clp-public-wifi) web page, you will see a chart showing Wifi usage for 2017. We can recreate that chart using Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Specify the year in a variable\n",
    "year = 2017\n",
    "\n",
    "# Select data for specified year, group by library/month, and aggregate by the adding together the minutes\n",
    "wifi_data_subset = wifi_data[wifi_data['Year'] == year].groupby([\"Name\", \"Month\"], as_index=False)[\"WifiMinutes\"].sum()\n",
    "\n",
    "# Reshape the data so it is easier to plot by Month\n",
    "reshaped_wifi_data_subset = wifi_data_subset.pivot_table(index=\"Month\", columns=\"Name\", values=\"WifiMinutes\")\n",
    "\n",
    "# plot the data\n",
    "ax = reshaped_wifi_data_subset.plot(figsize=(10,8),\n",
    "                                  title=f\"Carnegie Library of Pittsburgh {year} Wi-Fi Usage in Minutes\",\n",
    "                                  colormap=\"tab20\",\n",
    "                                  fontsize=14)\n",
    "# clean up the text\n",
    "ax.xaxis.label.set_size(16)\n",
    "ax.title.set_size(20)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.6,1))\n",
    "ax.ticklabel_format(style=\"plain\")\n",
    "\n",
    "# add the Month abreviations instead of numbers\n",
    "ax.set_xticks(ticks=range(1,13),labels=[\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Datasets To Find Neigborhoods\n",
    "\n",
    "To answer our motivating question, *What neighborhood uses the most WiFi?*, we need additional data. As we have seen above, there is no neighborhood information in the CLP Public WiFi data. In fact, there is very little information about any of the libraries.\n",
    "\n",
    "Fortunately, the Carnegie Library of Pittsburgh has posted another dataset on the WPRDC that has a bunch of information about each library location.\n",
    "\n",
    "The [Library Locations](https://data.wprdc.org/dataset/libraries) dataset includes a bunch of information about each CLP branch including \"address, phone number, square footage, and standard operating hours.\" Unfortunately, it does not include is neighborhood information, but it does have GPS locations for library!\n",
    "\n",
    "If we want to obtain neighborhood information for each library, we could spend some time and manually look up it up for each of the 19 locations. But we can also use Python programming to automate the process of looking up each library location and determining the neighborhood. To do this we will use the GPS location of each library location and look it up in a geographic dataset of Pittsburgh's neighborhoods.\n",
    "\n",
    "The WPRDC hosts another dataset, [Pittsburgh Neighborhoods](https://data.wprdc.org/dataset/neighborhoods2), which is a geographic dataset representing the spatial area of each of the 90 neighborhoods in Pittsburgh. What is useful about this dataset is that you can use computational methods to determine in which spatial area a particular GPS coordinate resides. What this means is we can use the GPS data from the library locations dataset and programmatically determine the neighborhood for each library location.\n",
    "\n",
    "By performing these computations we can add a new column to the library locations dataset that includes the neighborhood name. In this way we are **joining** two datasets together to enrich one of the datasets with information from the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Library Locations Data\n",
    "\n",
    "The WPRDC also hosts a [Library Locations](https://data.wprdc.org/dataset/libraries/resource/14babf3f-4932-4828-8b49-3c9a03bae6d0?view_id=f34cd02e-17eb-40aa-8f86-ae51968db84a) dataset that includes a bunch of information about each of the libraries in the CLP system.\n",
    "\n",
    "This dataset has 19 entries for each of the 19 libraries. For each library, we have the following pieces of information:\n",
    "\n",
    "- CLPID\n",
    "- Name\n",
    "- Address\n",
    "- City\n",
    "- Zip4\n",
    "- County\n",
    "- Phone\n",
    "- SqFt\n",
    "- The opening times for each day of the week\n",
    "- The closing times for each day of the week\n",
    "- Latitude\n",
    "- Longitude\n",
    "\n",
    "Most of the data in this dataset is location information for each of the locations, but it also includes some information about the library itself, namely how big and when it is open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load the library locations dataset into the variable library_data\n",
    "library_data = pandas.read_csv(\"clp-library-locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# display the first five rows of the library location data\n",
    "library_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Geographic Data\n",
    "\n",
    "* WPRDC publishes a dataset of Pittsburgh Neighborhoods.\n",
    "* [Pittsburgh Neighborhoods](https://data.wprdc.org/dataset/neighborhoods2)\n",
    "* This is a *geographic* dataset which means it requires some special Python libraries for opening and working with the data\n",
    "\n",
    "We need 3rd party library called [shapely](http://shapely.readthedocs.io) which will be used to encode GPS coordinates into geographic points. Then we will use the [Geopandas](https://geopandas.org/en/stable/) library to perform joining operations to determine the neighborhood for each GPS coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load up the necessary geographic libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Geographic Data Files\n",
    "\n",
    "You need to have a geographic dataset that represents the units of interest. Included in these materials is a [geojson](https://en.wikipedia.org/wiki/GeoJSON) file, `neighborhoods.geojson` that encodes all the neighborhoods in Pittsburgh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load the neighborhood geojson file\n",
    "pgh_neighborhoods = gpd.read_file(\"neighborhoods.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Geographic Data\n",
    "\n",
    "With the Pittsburgh neighborhood data loaded into Geopandas, we can now manipulate and visualize the geographic data just like our tabular data above. Each neighborhood in the dataset can be represented in a variety of ways. The default is to view the geographic data in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# take a peak at what these data look like\n",
    "pgh_neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular representations of geographic data are boring!\n",
    "\n",
    "The most common way to visualize geographic data is to make maps! The visualization below is just another representation of the neighborhoods' data. Note, the color is being used only to signify a different neighborhood, it doesn't have any particular value or meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the map using standard pandas plotting functions\n",
    "pgh_neighborhoods.plot(figsize=(10,10), cmap=\"tab20\"); #add semicolon to prevent ugly output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Neighborhood Names with Geographic Queries\n",
    "\n",
    "Pandas and Geopandas have a lot of handy built-in functions, but they does not include a function that can perform the geographic query we need to translate GPS coordinates into a neighborhood name. We need to do is use a special dataframe function, `apply()` that allows us to create our own custom function that will be *applied* to every row of the data. \n",
    "\n",
    "We need to create a python function that does the following:\n",
    "* Take a row of the library locations data as input\n",
    "* Convert the latitude and longitude columns of that row to a single \"Point\" object\n",
    "* Perform a lookup in the `pgh_neighborhoods` data to see if that Point, those coordinates exists in our geographic dataset\n",
    "* Return the name of the neighborhood if it exists or return `NaN`\n",
    "\n",
    "By \"applying\" our new function to the library location data, a new column of neighborhood names  will be added to the data. While we only have 19 library locations and manually adding this column would not be too difficult, what if we had 100 or 1000 library locations? Automating this process allows us to process much larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create a function that we will supply to apply\n",
    "def reverse_geolocate_neighborhood(row):\n",
    "    \"\"\"Given a row, grab the latitude and longitude columns and\n",
    "    return the neighborhood name (or nan for locations outside the dataset).\"\"\"\n",
    "\n",
    "    # get the latitude and longitude\n",
    "    latitude = float(row['Lat'])\n",
    "    longitude = float(row['Lon'])\n",
    "\n",
    "    # create a shapely point from the GPS coordinates\n",
    "    location = Point(longitude, latitude)\n",
    "\n",
    "    # make a query mask and query the data on that location\n",
    "    location_query = pgh_neighborhoods['geometry'].contains(location)\n",
    "    result = pgh_neighborhoods[location_query]\n",
    "\n",
    "    # if the location isn't in dataset it will be empty\n",
    "    if result.empty:\n",
    "        # location isn't within Pittsburgh, return not-a-number\n",
    "        return nan\n",
    "    else:\n",
    "        # return a string of the \"hood\" where the point was located\n",
    "        return result.iloc[0]['hood']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can give our `reverse_goelocate_neighborhood` function as a parameter to the [dataframe apply](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) function and Pandas process every row of the data using our custom function. This will create a new column with all the neighborhood names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# perform reverse geocoding with every row in the library data\n",
    "# save results as a new column in our tree dataframe\n",
    "library_data['neighborhood'] = library_data.apply(reverse_geolocate_neighborhood, axis=1)\n",
    "\n",
    "# display the update librar data\n",
    "library_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has modified our local copy of the library locations dataset and added a \n",
    "new column (scroll all the way to the right to see it above) indicating in which neighborhood each CLP library branch is located.\n",
    "\n",
    "Now all we have left to do is *join* our enriched library locations data with the Public Wifi Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the Library Locations and Public Wifi Data\n",
    "\n",
    "We now have all of the information we need to answer the question, what neighborhood uses the most Wifi in Pittsburgh. However, the data we need to answer this question is in two separate datasets. What we need to do is *join* the two dataset together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# look at the public wifi data\n",
    "wifi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# look at the library data\n",
    "library_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When joining together two separate datasets, you need to consider several factors. First, are there any shared data values, that is, are there columns that are the same or have the same values in both datasets. For the Wifi and Library Locations data we can see there are two shared columns: `CLPID` and `Name`. The data values in the `Name` columns are the same in both datasets meaning we can connect rows from one dataset with rows from another dataset. Second, we need to consider the *thing* that each row represents within each dataset and what kinds of relationships those things have with each other. In the Public Wifi datasets each row represents the wifi usage at a particular library location for a particular month. In the Library Locations dataset each row represents one of the 19 CLP library locations. The question we have to consider when merging two datasets together is what relationship one row in the first dataset has with one or more rows in the second dataset.\n",
    "\n",
    "### Cardinality\n",
    "\n",
    "In the language of data modeling, considering these relationships is called [*cardinality*](https://en.wikipedia.org/wiki/Cardinality_(data_modeling)). There are several ways we can talk about the cardinality, i.e. the relationship between the rows of these two datasets:\n",
    "- One-to-One : A row in one dataset correspond only to a single row in the other dataset. A person has only one library card at their local public library.\n",
    "- One-to-Many: A row in one dataset corresponds to multiple rows in the other dataset. A person can check out many books from their local public library.\n",
    "- Many-to-Many: Multiple rows in one dataset correspond to multiple rows in the other dataset. Many people check out many different books from their local public library.\n",
    "\n",
    "In our case, we have a one-to-many relationship between Library Locations and Public Wifi datasets. Each row in the library locations dataset represents one single library. That one library is represented in many rows in the Public Wifi Data because each row represents the amount of wifi usage, in a month, at a particular library location. We can see this in the CLP Public Wifi data because the library names get repeated over and over, whereas in the library locations data a library name only appears once.\n",
    "\n",
    "### Merging Library Location into Public Wifi data\n",
    "\n",
    "Merging operations are complicated. It is not simply a matter of copying columns from one dataset and pasting into another, we need to make sure the new columns align with the data in each row. Fortunately, we can use the `merge` operation in our Python code to automatically join the two datasets together with the correct alignment.\n",
    "\n",
    "When merging two datasets in a one-to-many relationship, we can decide what shape the new dataset will take. It could mirror the shape of either one of the previous dataset or be a union of both. In our case, we want our merged dataset to have the same shape as the Public Wifi data, but with location information added. This means our new dataset will have the same number of rows as the existing Public Wifi data, but with additional columns derived from the library locations dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# merge the two datasets and save the results into a new variable\n",
    "wifi_with_location_info = pandas.merge(wifi_data, library_data)\n",
    "wifi_with_location_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new dataset that looks like the Public Wifi dataset, but it now includes location information for each row. If you look at the output above, you will notice there is a lot of repeated values. This is because we did a \"one to many\" join, that is, the data from one row in the Library Locations dataset corresponds to many rows in the Public Wifi datasets. While this results in a lot of duplicated data values, if you scroll all the way to the right in the output above you will see there is now a `neighborhood` column with a value for each of the Wifi session.\n",
    "\n",
    "We now have all the information we need to answer our motivating question in one single dataset! The final step is to *aggregate* the data by performing calculations to determine the totals per neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating with Split-Apply-Combine\n",
    "\n",
    "When we performed an aggregation operation earlier in this notebook we just computed the sum total number of sessions and minutes across the whole dataset. By adding neighborhood information to the dataset, we can now perform the aggregation operation not just on all the data, but on specific subsets or *groups* of the data. In our case, we want to group rows by the neighborhood and then aggregate the total values for each of these groups.\n",
    "\n",
    "In the language of data analysis, this set of operations is known as [*split, apply, and combine*](https://pandas.pydata.org/docs/user_guide/groupby.html):\n",
    "- Splitting the data into groups based on some criteria.\n",
    "- Applying a function to each group independently.\n",
    "- Combining the results into a data structure.\n",
    "\n",
    "We can use this split, apply, combine operation to answer our original question by calculating the total wifi usage per neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create new dataset of total wifi usage per neighborhood using the groupby operation\n",
    "totals_per_neighborhood = wifi_with_location_info.groupby(\"neighborhood\",as_index=False)[[\"WifiSessions\", \"WifiMinutes\"]].sum()\n",
    "totals_per_neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python code above did a lot of calculations in very little code. Following the split-apply-combine paradigm we can explain what the code was doing:\n",
    "\n",
    "- **Split** - Separate the `wifi_with_location_info` dataset into 19 groups, one group for each library location's neighborhood.\n",
    "- **Apply** - Select just the `WifiSessions` and `WifiMinutes` columns and then calculate the sum total value of those two columns for each of the 19 groups.\n",
    "- **Combine** - Create a new dataset with 19 rows, one row for each of the grouping values(i.e. neighborhood) and the sum total value for the two columns (Wifi Sessions and Wifi Minutes) for each group.\n",
    "\n",
    "After running the code above, we have created a new dataset that contains the answer to our question. However, it is a bit difficult to read so the final step in our computations will be to sort the data based on the aggregated `WifiMinutes` per neighborhood column so we can see who uses the most Wifi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# sort data by WifiMinutes\n",
    "totals_per_neighborhood.sort_values(\"WifiMinutes\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering our Data Drive Question\n",
    "\n",
    "Now we can see the answer to our question: *What neighborhood in Pittsburgh uses the most Wifi at a Public Library?*\n",
    "\n",
    "The answer is North Oakland!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Data\n",
    "\n",
    "As one final step, we should save our new dataset to disk. Now, we can use the dataset without having to re-perform the computations in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# write the totals per neighborhood dataframe as a comma separated file\n",
    "totals_per_neighborhood.to_csv(\"totals_per_neighborhood.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
